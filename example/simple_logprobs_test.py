#!/usr/bin/env python3
"""
ä¿®æ­£ç‰ˆ: get_top_response_tokens ãƒ†ã‚¹ãƒˆ - æŒ™å‹•ã®é•ã„ã‚’è§£æ±º
"""

import math
import requests
from dataclasses import dataclass
from typing import List, Optional

@dataclass
class Token:
    token: str
    prob: float
    top_candidates: Optional[List['Token']] = None

def get_top_response_tokens_from_llama_cpp(prompt: str = "The capital of Japan is", max_tokens: int = 100, temperature: float = 0.7) -> List[Token]:
    """llama.cppã‹ã‚‰ç›´æ¥logprobsã‚’å–å¾—ã—ã¦Tokenå½¢å¼ã«å¤‰æ›"""
    
    print(f"ğŸ”„ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: '{prompt}' (max_tokens={max_tokens}, temp={temperature})")
    
    # llama.cpp Chat APIãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼ˆè¨­å®šã‚’æŸ”è»Ÿã«ï¼‰
    payload = {
        "model": "llama",
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": max_tokens,
        "temperature": temperature,
        "logprobs": True,
        "top_logprobs": 3
    }
    
    try:
        response = requests.post(
            "http://localhost:8081/v1/chat/completions",
            json=payload,
            timeout=30
        )
        response.raise_for_status()
        data = response.json()
        
        # ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆè¡¨ç¤º
        content = data['choices'][0]['message']['content']
        print(f"ğŸ“ ç”Ÿæˆçµæœ: '{content}'")
        print(f"ğŸ“Š ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {data['usage']['completion_tokens']}")
        
        # logprobsã‹ã‚‰Tokenå½¢å¼ã«å¤‰æ›
        logprobs_data = data['choices'][0]['logprobs']['content']
        tokens = []
        
        for token_data in logprobs_data:
            # ãƒ¡ã‚¤ãƒ³ãƒˆãƒ¼ã‚¯ãƒ³
            main_prob = math.exp(token_data['logprob'])
            
            # ä¸Šä½å€™è£œ
            candidates = []
            for candidate in token_data.get('top_logprobs', []):
                cand_prob = math.exp(candidate['logprob'])
                candidates.append(Token(candidate['token'], cand_prob))
            
            # Tokenã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ
            token = Token(
                token=token_data['token'],
                prob=main_prob,
                top_candidates=candidates
            )
            tokens.append(token)
        
        return tokens
        
    except Exception as e:
        print(f"âŒ Chat API ã‚¨ãƒ©ãƒ¼: {e}")
        return []

def get_completion_tokens(prompt: str = "The capital of Japan is", n_predict: int = 100, temperature: float = 0.7) -> List[Token]:
    """Completion APIã§logprobsã‚’å–å¾—"""
    
    print(f"ğŸ”„ Completion API: '{prompt}' (n_predict={n_predict}, temp={temperature})")
    
    payload = {
        "prompt": prompt,
        "n_predict": n_predict,
        "temperature": temperature,
        "n_probs": 3  # Completion APIã®logprobs
    }
    
    try:
        response = requests.post(
            "http://localhost:8081/completion",
            json=payload,
            timeout=30
        )
        response.raise_for_status()
        data = response.json()
        
        # ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆè¡¨ç¤º
        content = data.get('content', '')
        print(f"ğŸ“ ç”Ÿæˆçµæœ: '{content}'")
        print(f"ğŸ“Š ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {data.get('tokens_predicted', 0)}")
        
        # Completion APIã®logprobså‡¦ç†
        completion_probs = data.get('completion_probabilities', [])
        tokens = []
        
        for token_data in completion_probs:
            # ãƒ¡ã‚¤ãƒ³ãƒˆãƒ¼ã‚¯ãƒ³
            main_prob = math.exp(token_data['logprob'])
            
            # ä¸Šä½å€™è£œ
            candidates = []
            for candidate in token_data.get('top_logprobs', []):
                cand_prob = math.exp(candidate['logprob'])
                candidates.append(Token(candidate['token'], cand_prob))
            
            # Tokenã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ
            token = Token(
                token=token_data['token'],
                prob=main_prob,
                top_candidates=candidates
            )
            tokens.append(token)
        
        return tokens
        
    except Exception as e:
        print(f"âŒ Completion API ã‚¨ãƒ©ãƒ¼: {e}")
        return []

def display_tokens(tokens: List[Token]):
    """Tokenæƒ…å ±ã‚’è¦‹ã‚„ã™ãè¡¨ç¤º"""
    
    if not tokens:
        print("âš ï¸  ãƒˆãƒ¼ã‚¯ãƒ³ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return
    
    print(f"\nğŸ“Š ãƒˆãƒ¼ã‚¯ãƒ³åˆ†æçµæœ ({len(tokens)}å€‹)")
    print("=" * 50)
    
    for i, token in enumerate(tokens):
        print(f"\nğŸ¯ ãƒˆãƒ¼ã‚¯ãƒ³ {i+1}: {repr(token.token)}")
        print(f"   ç¢ºç‡: {token.prob:.4f} ({token.prob*100:.2f}%)")
        
        if token.top_candidates:
            print(f"   ä¸Šä½å€™è£œ:")
            for j, candidate in enumerate(token.top_candidates):
                print(f"     {j+1}ä½: {repr(candidate.token)} - {candidate.prob:.4f} ({candidate.prob*100:.2f}%)")

def compare_chat_vs_completion(prompt: str, max_tokens: int = 50, temperature: float = 0.7):
    """Chat API vs Completion APIã®ç›´æ¥æ¯”è¼ƒ"""
    
    print(f"\nğŸ†š APIæ¯”è¼ƒãƒ†ã‚¹ãƒˆ: '{prompt}'")
    print(f"è¨­å®š: max_tokens={max_tokens}, temperature={temperature}")
    print("=" * 60)
    
    # Chat API
    print("ğŸ“± Chat API:")
    chat_tokens = get_top_response_tokens_from_llama_cpp(prompt, max_tokens, temperature)
    if chat_tokens:
        # æœ€åˆã®5ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿è¡¨ç¤ºï¼ˆã‚¹ãƒšãƒ¼ã‚¹ç¯€ç´„ï¼‰
        display_tokens(chat_tokens[:5])
    
    print("\n" + "-" * 30)
    
    # Completion API
    print("ğŸ“ Completion API:")
    completion_tokens = get_completion_tokens(prompt, max_tokens, temperature)
    if completion_tokens:
        # æœ€åˆã®5ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿è¡¨ç¤º
        display_tokens(completion_tokens[:5])
    
    print("\n" + "=" * 60)

def test_various_prompts():
    """è¤‡æ•°ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ãƒ†ã‚¹ãƒˆ - è¨­å®šã‚’çµ±ä¸€"""
    
    test_prompts = [
        "The capital of Japan is",
        "1 + 1 =", 
        "Hello, my name is",
        "ä»Šæ—¥ã®å¤©æ°—ã¯"
    ]
    
    # è¨­å®šã‚’çµ±ä¸€ã—ã¦æ¯”è¼ƒ
    MAX_TOKENS = 20  # çŸ­ã‚ã«è¨­å®š
    TEMPERATURE = 0.7
    
    for prompt in test_prompts:
        print(f"\n{'='*60}")
        print(f"çµ±ä¸€è¨­å®šãƒ†ã‚¹ãƒˆ: {prompt}")
        print(f"{'='*60}")
        
        # Chat APIã§ãƒ†ã‚¹ãƒˆ
        print("ğŸ“± Chat API (çµ±ä¸€è¨­å®š):")
        tokens = get_top_response_tokens_from_llama_cpp(prompt, MAX_TOKENS, TEMPERATURE)
        if tokens:
            display_tokens(tokens[:3])  # æœ€åˆã®3ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿
            
            # æœ€é«˜ç¢ºç‡ãƒˆãƒ¼ã‚¯ãƒ³ã®ä¿¡é ¼åº¦ãƒã‚§ãƒƒã‚¯
            max_prob = max(token.prob for token in tokens[:3])
            if max_prob > 0.8:
                print(f"\nâœ… é«˜ä¿¡é ¼åº¦äºˆæ¸¬ (æœ€å¤§ç¢ºç‡: {max_prob:.3f})")
            elif max_prob > 0.5:
                print(f"\nâš¡ ä¸­ç¨‹åº¦ä¿¡é ¼åº¦ (æœ€å¤§ç¢ºç‡: {max_prob:.3f})")
            else:
                print(f"\nâš ï¸  ä½ä¿¡é ¼åº¦äºˆæ¸¬ (æœ€å¤§ç¢ºç‡: {max_prob:.3f})")
        else:
            print("âŒ ãƒˆãƒ¼ã‚¯ãƒ³å–å¾—å¤±æ•—")

def analyze_behavior_differences():
    """æŒ™å‹•ã®é•ã„ã‚’è©³ç´°åˆ†æ"""
    
    print(f"\nğŸ” æŒ™å‹•ã®é•ã„è©³ç´°åˆ†æ")
    print("=" * 60)
    
    test_prompt = "ä»Šæ—¥ã®å¤©æ°—ã¯"
    
    # 1. çŸ­ã„ç”Ÿæˆã§ã®æ¯”è¼ƒ
    print("ğŸ”¬ ãƒ†ã‚¹ãƒˆ1: çŸ­ã„ç”Ÿæˆ (5ãƒˆãƒ¼ã‚¯ãƒ³)")
    compare_chat_vs_completion(test_prompt, max_tokens=5, temperature=0.7)
    
    # 2. é•·ã„ç”Ÿæˆã§ã®æ¯”è¼ƒ
    print("ğŸ”¬ ãƒ†ã‚¹ãƒˆ2: é•·ã„ç”Ÿæˆ (50ãƒˆãƒ¼ã‚¯ãƒ³)")
    compare_chat_vs_completion(test_prompt, max_tokens=50, temperature=0.7)
    
    # 3. ä½æ¸©åº¦ã§ã®æ¯”è¼ƒ
    print("ğŸ”¬ ãƒ†ã‚¹ãƒˆ3: ä½æ¸©åº¦ (ç¢ºå®šçš„)")
    compare_chat_vs_completion(test_prompt, max_tokens=20, temperature=0.1)

if __name__ == "__main__":
    print("ğŸš€ ä¿®æ­£ç‰ˆ: get_top_response_tokens ãƒ†ã‚¹ãƒˆ")
    print("ğŸ¯ ç›®çš„: Chat API vs Completion API ã®æŒ™å‹•ã®é•ã„ã‚’è§£æ˜")
    
    # ã‚µãƒ¼ãƒãƒ¼ç¢ºèª
    try:
        health = requests.get("http://localhost:8081/health", timeout=5)
        if health.status_code != 200:
            print(f"âŒ ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼: {health.text}")
            exit(1)
        print("âœ… llama.cpp ã‚µãƒ¼ãƒãƒ¼æ¥ç¶šOK\n")
    except Exception as e:
        print(f"âŒ ã‚µãƒ¼ãƒãƒ¼æ¥ç¶šå¤±æ•—: {e}")
        exit(1)
    
    # åŸºæœ¬ãƒ†ã‚¹ãƒˆï¼ˆå…ƒã®è¨­å®šã§ï¼‰
    print("ğŸ“‹ åŸºæœ¬ãƒ†ã‚¹ãƒˆï¼ˆé•·ã„ç”Ÿæˆï¼‰")
    tokens = get_top_response_tokens_from_llama_cpp("The capital of Japan is", 100, 0.7)
    if tokens:
        display_tokens(tokens[:5])  # æœ€åˆã®5ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿
    
    # æŒ™å‹•ã®é•ã„ã‚’åˆ†æ
    analyze_behavior_differences()
    
    # çµ±ä¸€è¨­å®šã§ã®ãƒ†ã‚¹ãƒˆ
    print(f"\nğŸ“‹ çµ±ä¸€è¨­å®šã§ã®è¤‡æ•°ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ã‚¹ãƒˆ")
    test_various_prompts()
    
    print(f"\nğŸ ãƒ†ã‚¹ãƒˆå®Œäº†ï¼")
    print("ğŸ’¡ çµè«–: APIã®ç¨®é¡ã¨è¨­å®šã«ã‚ˆã£ã¦å¿œç­”ãŒå¤‰ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª")
